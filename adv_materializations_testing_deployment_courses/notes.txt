####################################################################################################################
#### Fundamentals course
dbt run
    - run a specific model: dbt run -m dim_customers (where dim_customers is the name of the model)
    - run a specific model, option 2: dbt run --select dim_customers

dbt test
    - run the sql against tables or views as necessary and tell us the results of the test

dbt build
    - runs dbt run && dbt test ... ish
    - performs dbt run on the first "layer" of models in DAG order, then performs dbt test on that first "layer"; performs dbt run on the second "layer", tests second layer, etc.
    - pro: if perform "dbt run" and then "dbt test", and a test fails upstream, you will have just built the data based on a model dependent on failing data!

dbt docs generate
    - goes through full project, builds documentation site

dbt docs serve

dbt source freshness
    - as long as you have the freshness: block in your source <source_name>.yml file you can run dbt source freshness command and warn or error if we have not seen new data

# testing
    - singular vs generic tests
    - singular test
        - write a custom sql test file in the tests folder
    - generic tests
        - come in four flavors: unique, not_null, accepted_values, relationships
        - can be run by declaring them in a yml file eg stg_jaffle_shop.yml
    - you can test sources as well (not just models)

# documentation
- documentation can be written directly into the yml files where we declared the tests for our models using the description: line
- alternatively, you can use doc blocks that are a separate markdown file
    - in that description: line you'll reference the name of the doc block, not the name of the markdown file the doc block lives in (this enables multiple dock blocks in one md file)
- this all works with sources, too



####################################################################################################################
#### Refactoring course
# refactoring process:
step 1: Migrating legacy code 1:1
step 2: translating hard coded table references
step 3: choosing a refactoring strategy
    option 1: overwriting that legacy file we copied over, editing the sql in the file we copied over
    option 2: merge in one small slice at a time from the legacy file (the route she chooses) into new model files as necessary
step 4: CTE groupings and cosmetic cleanups
    - import CTEs
    - logical CTEs (ie subqueries)
    - final CTEs
    - simple select statement
step 5: centralizing transformations & splitting up models
    staging models
    CTEs or intermediate models
    final model
step 6: auditing

dbt deps - installs packages listed in the packages.yml file and their dependencies
dbt run - runs the whole project
dbt test - runs the tests in the tests folder
dbt compile - compiles the models, analyses, and tests code in the Target folder
dbt run -m <model_name> - runs a specific model

dbt docs generate - generates documentation files (manifest.json and catalog.json )
dbt docs serve - serves the generated documentation files



####################################################################################################################
#### Jinja, Macros, Packages course
# JINJA
## single vs double curly braces

### single-curly ie {} is using jinja templating
### double-curly is telling jinja to print something out
Example:
{% for i in range(5) %} < p > {{ i }} < / p > {% endfor %}
This will print out 0, 1, 2, 3, 4
## setting variables in Jinja
Example:
{% set name = "Tim" %}
-- This will set the variable name to Tim
{{ name }}
-- This will print out Tim


## numbers
-- Example:
{% set age = 20 %}


## lists
-- Example:
{% for name in names %}
{% set names = ["Tim", "Joe", "Sam"] %}
    of all names,
-- This will set the variable names to a list of Tim, Joe, and Sam
    my favorite is {{ name }}
    {% endfor %}
    -- This will print out Of all names, my favorite is ...
    

## more practice with Jinja
{% set current_temperature = 45 %}
{% if current_temperature < 65 %}
    Time for a cappuccino
{% else %}
    Time for iced coffee
{% endif %}


## To remove the whitespace, use the minus sign
{% for i in range(5) -%}
    {{ i }}
{%- endfor %}


## Dictionaries
Example:
{% set person = {"name": "Tim", "age": 45} %}
This will set the variable person to a dictionary with the keys name and age
{{ person["name"] }}
This will print out Tim
{{ person["age"] }}
This will print out 45



# MACROS
- Macros are re-usable throughout the project (define them in the macros folder)
- Macros are a feature of Jinja, not unique to dbt
-   Recall: Jinja is a templatized language that runs Python in the background
- Think about DRY (dont repeat yourself) code vs Readability.  Be somewhere in the middle of those two opposing ideas
- to run a macro standalone (straight from the command line):
dbt run-operation <name_of_macro>
eg:
dbt run-operation grant_select
- to pass info back to the terminal or output in general use the log() function as outlined in grant_select.sql file


# PACKAGES
- Import models and macros that have already been written. Leverage modeling of common sources.
- Packages live in the packages.yml file
- to install packages in the yml file run dbt deps
- three options
-   the way the package is called on hub.getdbt.com
-   you can install packages from a private repository by connect directly to git in the packages.yml file!
-   you can install packages from a local file
- packages contain much more than just models
-   Seeds, analyses, etc

# Random useful functions
- run_query() is a wrapper that takes statements and runs them in the respective DWH
- log() log a line of text to the logs in dbt.
- target() contains information about your connection to the warehouse.


####################################################################################################################
#### Analyses and Seeds course
# Analyses
- sql files in the analyses folder which support Jinja and can be compiled. 
- Will NOT be run with dbt run, but will be compiled with the models etc by running
dbt compile
- They're not tests or models.

# Seeds
- csv files that live in the data folder
- build a table from small amounts of data in CSVs by running:
dbt seed
- models can use the ref() function to reference seeds
- you can test the seed same way we would test models 
-   see the seeds/schema.yml file for example configuration
-   and then run dbt test --models employees



####################################################################################################################
#### Advanced Materializations
# Five materializations overview
- ways dbt builds the models we write
- materializations can be configured in the project.yml file or at the top of the model.sql file in the {{ config() }} block
1. tables - the data itself is saved as a table. 
    The table exists in the DB.
2. views - the query that builds the table is saved in the database and when you run select * from some_view the query is re-run. 
    The view exists in the DB, but not as data rather it exists as a query.
3. ephemeral - the query is not saved in the database, it imports the select statement as a CTE.  
    The ephemeral model does NOT exist in the DB, its a reusable code snippet.  Ie cannot select * from the ephemeral model.
4. incremental - looks at the underlying data and asks "which of these are new?" and adds the new ones into the destination table
    think of incremental models as an upgrade path
        Ie start with views. When the views are too long to query, switch to tables (longer to build, shorter to query)
        When tables take too long to build, switch to incremental models
    what we're doing ...
        - tell dbt to add new rows instead of recreating the table (materialized = 'incremental' does this)
        - How do we ID the new rows?  we apply a where clause on new runs that specifies only data that has arrived since the last time we updated this table
    steps dbt follows:
        1. build a temporary table with the new rows (where filter)
        2. insert those new rows into the existing table, replacing existing rows
        3. drop the temp table
    tell incremental model to replace existing rows with "unique_key = ..."
    incremental models ...:
        - are approximately correct (but not completely, mainly bc late arriving data)
        - introduce complexity
        - folks try to incorporate logic to increase the accuracy of what is built, but then that slows down and negates the benefits of incremental (performance)
5. snapshots - look at underlying data and look for whats changed.  If something as changed its brought in as a NEW ROW. Think SCDs.
    - live in a folder called snapshots; sql files just like models
    - capture Type 2 SCDs
    - they are run separately from models, seeds, etc.  Ie run by running: dbt snapshot
    - adds four columns: 
        - a new unique identifier
        - updated_at
        - valid_from
        - valid_to
    - this is NOT a full copy of the table every day, we're recording timestamps for periods of validity
    - use the timestamp strategy where possible, but sometimes its too sensitive (what if we only care about one column? we can use check_cols='')
    
####################################################################################################################
#### Advanced testing course

## Introduction to Advanced Testing:
# What are tests?
- assertions you make about your model
- sql statements that seek to grab records that dissaprove your assertion (unique records test grabs records that are not unique)

# Testing techniques
- interactive / ad hoc queries (scratch pad that has the query saved)
- standalone saved query (save the query to a file)
- expected results (additional logic to only surface failures and adds that context someplace like the column name)
- test on a schedule --> take action

# What makes a good test?
- automated
- fast
- reliable
- informative
- focused (if you have a test that checks for uniqueness and not-null, and it fails ...which one caused the failure?  therefore its a bad test)

# What to test and why
- test on one database object
    - Example tests: unique, not_null, etc.
    - ONE object
- test how one database object referes to another DB object
    - Example tests: relationships, etc.
    - MULTIPLE objects and how they relate to each other
- test something unique about your data
    - NOT uniqueness a la primary keys.  What they mean is tests that are unique to you and your data.  Think: business logic edge cases.
    - Eg. making sure you don't refund a customer more than they paid by testing for payments <= refund; something like that
    - remember: if your test returns results, the test failed.
- test freshness of your raw source data
    - Example tests: freshness, etc. (dbt source freshness)
    - see if your raw data sources have been updated. Prevents objects from being created if the source data has not been updated.
    - ONLY works on your raw data
- temporary testing while refactoring
    - Example tests: audit_helper
    - auditing your changes while in development

# Measuring and enforcing test coverage
- generic tests: unique, not_null, relationships, accepted_values
- tests can be run on columns in a table or the table as a whole
- tests can also be run on the project as a whole (in the dbt_project.yml file)
    - these tests act on the .yml files.
    - eg: test documentation , test whether tests exist, etc.
    - to run: dbt run-operation or use a github action
- runs through demo of requiring and running tests at the project level (packages.yml file)

## Test Deployment:
# When to test?
- Manual vs Automated tests
- Manual
    - when you first run a project and during development
        - dbt test or dbt build
- automated
    - when you run dbt on a schedule (deployment!)
        - dbt build
    - when you want to merge your code (git CI checks)
        - dbt build --models state:modified+
    - test in QA branch before your dbt code reaches main
        - dbt build

# Testing commands
- dbt test will test on models, sources, snapshots, and seeds
- dbt test --select <object_name>   <-- this model only
- dbt test --select object_name>+  <-- this model and everything downstream
- dbt test --select <folder>.<sub_folder>.*   <-- all tests in the subfolder
- dbt test --select <some_package>.*    <-- all tests in a specific package
- commas are the intersection of two things, eg:
    - dbt test --select orders,test_type:singular    <-- runs the singular tests on the orders model
- dbt test --select source:*   <-- runs all tests on the sources
- dbt test --exclude source:*   <--- runs all tests except the sources
- dbt build --fail-fast    <-- runs models, seeds, sources, snapshots as soon as any given object becomes available;
    --fail-fast flag tells dbt to immediately abort when anything fails

# Storing test failures in the DB
- dbt test --select <model_name> --store-failures
    - spins up a new schema and saves the records that caused the test to fail.  Neat!
    - note: test's results will always REPLACE previous failures, do NOT use for tracking test failures over time
    - to do that:
        - there are some macros you can use to track success/failures over time
            - store_test_results macro: https://www.getdbt.com/blog/dbt-live-apac-tracking-dbt-test-success
        - or use packages, like dbt_artifacts

## Custom Tests:
- tests are just SQL, so of course we can write our own!
- two custom tests: singular or generic tests
- singular test
    - a special test toward only one model
- custom generic tests
    - a singular test that you want to apply to multiple models
    - use jinja, much like a macro
- overwriting native tests
    - you can overwrite a native test by creating a new test by the exact same name as the native version


## Tests in Packages:
- a few particularly helpful packages:
    - audit_helper
        - when making significant updates to models and want to ensure those changes don't change the data
            - ie compare data from two tables and returns the % of rows that don't match
        - save the compare_relations test(s) in the analysis folder will prevent this from being run every time the models are built in prod
            - bc this is more commonly run in dev
        - compare_column_values test(s) is a macro so it will be saved in the macros folder
            - spits out the mismatches between two tables by column
    - dbt_utils
    - expectations
        - eg test for validity of our expectations against certain columns
- dont forget to add the test(s) to your model.yml files!
    - they did NOT include the model.yml file(s) you'd expect, like __core.yml in the models/marts/core folder


## Test Configurations:
- you can specify configurations in the .yml context like...
    - severity: "warn" instead of throwing an error
    - or, keep severity to "error" but if value exceeds 100, for example
- testing the 'orders' model's generic tests:
    dbt test --select orders,test_type:generic
- additional configs for test-level tests:
    - where: ...
    - limit: ...
    - store_failures: true
        - provides the query that you can run to see the failing records
- additional configs for project-level tests!


####################################################################################################################
## Advanced Deployment
# Environment, jobs, and run results
- Environment:
    - dbt version
    - git branch
    - data location (ie schema)
- Two environment types in dbt Cloud IDE
    - development (only one for every project)
    - deployment (as many as you need)
- Within a deployment environment we have jobs:
    - sequence of dbt commands like dbt run, dbt test, dbt build
    - scheduled or triggered
- Runs are the implementation of a specific job
- Run results are the results of job runs ... pretty straight forward

# Deployment architecture in dbt cloud
- two ways of promoting code in the repository:
    - Direct promotion - one trunk
        - feature branches are merged into one main/master branch
        - Pro: simpler
        - Con: you're branching and merging directl from/to the main branch. If something goes wrong, it goes very wrong.
        - Setup in dbt cloud
            - 
    - Indirect promotion - many trunks
        - feature branches are merged into intermediary branches for QA before being PR'd into main/master
        - Pro: another layer of QA
        - Con: more complex
        - Setup in dbt cloud
            - 

# Job structures
- DAG overview
    - green node: sources
    - orange node: exposure
    - blue node: everything else (tests, models, etc)
- Threads: how many concurrent parts of your job you want to run at any given time
- dbt build
    - runs and tests all nodes (sources, models, tests, seeds, etc etc)
    - but will not run models downstream of tests until the test passes! 
- Different job types:
    - Standard job - build entire project, leverage incremental logic, typically daily
    - Full refresh job - build entire project, rebuild the entire model (even if incremental), typically weekly
    - Time sensitive job - build a subset of your DAG (not the entire project), helpful to refresh models for certain parts of the business
    - Fresh rebuild - check if sources have been updated (by way of a freshness check), if yes: build downstream models
- Common pitfall: too many one-off very specific jobs. Here are some tips for overcoming:
    - tags:
        - use tags to tag models with their frequency
        - Eg a "morning" tag, an "evening" tag, a "daily" tag, a "weekly" tag, etc
    - exposures: 
    - sources: build from the source outward
    - folder restructure:
- in the dbt build --select ..... command:
    - a UNION is a SPACE (ie, a space between models runs everything but only once if they share common parents / children)
    - an INTERSECTION is a comma (ie, a comma between models runs only those parent / child models they share)
- running a model build on nodes where the source data has been refreshed:
    - dbt build --select source_status:fresher+

# Orchestration in dbt Cloud
- three ways to trigger jobs:
    - on a schedule
        - nice gui or box to enter CRON
    - via webooks
    - via API calls

- review past runs:
- artifacts: 
    - objects generated as a result of the command 
        - (eg SQL DDL creating schema, table, etc)
        - docs generated

- ways to trigger jobs in dbt cloud:
    - first, you can use your API token or create service token(s) (if enabled on your account)
    - now, you can trigger using...
        - curl: a curl request like
            curl --request POST ...
        - python: using requests package and building out your python script with the necessary variables. From the terminal you can then run 
            python <file_name.py>
        - dbt-cloud-cli: a command like...
            dbt-cloud job run --account-id <account_id> --job-id <job_id> --cause....
            - a GitHub repo exists for this

- coordinating different jobs:
    - they talked about using CRON, but I'd definitely recommend Airflow instead
    
# Continuous Integration in dbt Cloud
- A traditionally software DEVOPS practice of employing automated builds and tests in your code base BEFORE merging into production
- run a job called: run on pull request (CI)
    - rebuilds the DAG when a PR is made
    - if it doesn't pass, it won't let the merge happen
    - but, do we REALLY need a full-rebuild of the entire DAG every time we make a PR? AND every time we commit to an already approved PR?
        - now enter...slim CI
- run a job called: run on pull request (Slim CI)
    - looks at two code bases, ID whats changed, and rebuild just the nodes that changed as well as those that are downstream
    - how? run:
        dbt run --select state:modified+
        dbt build --select state:modified+
- defer to a previous run state?
    - determine which job run to build off of
    - at least one successful standard job is required (to defer to) for Slim CI jobs

# Custom Environment and Job Behavior
- using the target Jinja variable
    - available in any code block to grab information about your connection to the DWH
- leveraging environment variables
    - step 1: overwrite the default macro by creating a new macro .sql file of the same name
    - environment variables are parameters that are set outside the code
